{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üìä Formula 1 Statistical Analysis (2010-2024)\n\n## Project Overview\n**Research Question:** What factors significantly influence race performance in modern Formula 1?\n\n**Dataset:** \n- Period: 2010-2024 (Modern Hybrid Era)\n- Sample Size: 6,436 race results from 305 races\n- Sources: races.csv, results.csv, drivers.csv, constructors.csv\n\n**Courses:** TU155 (Statistics) & DSI204 (Data Science & Analytics)\n\n---\n\n## Methodology\n\n### Data Science Best Practices Applied:\n\n1. **Reproducibility**\n   - Fixed random seed (RANDOM_STATE = 42)\n   - Version control ready\n   - All analysis steps documented\n\n2. **Data Quality**\n   - Comprehensive data validation\n   - Missing value analysis\n   - Outlier detection\n   - Duplicate checking\n\n3. **Statistical Rigor**\n   - Assumptions testing for all statistical tests\n   - Effect size reporting (Cohen's d, eta-squared, Cram√©r's V)\n   - Confidence intervals provided\n   - Multiple comparison corrections (Tukey HSD)\n\n4. **Model Validation**\n   - Train-test split (80/20)\n   - Cross-validation (5-fold)\n   - Overfitting checks\n   - Regression diagnostics (linearity, homoscedasticity, normality)\n   - Multicollinearity detection (VIF)\n\n5. **Proper Metrics**\n   - Classification: Accuracy, Precision, Recall, F1, AUC-ROC\n   - Regression: R¬≤, Adjusted R¬≤, RMSE, MAE\n   - Hypothesis Testing: p-values, test statistics, effect sizes\n\n---\n\n## Analysis Structure\n\n1. **Environment Setup** - Libraries, configuration, reproducibility\n2. **Data Loading & Validation** - Quality checks, missing values\n3. **Data Cleaning & Feature Engineering** - Derived variables, transformations\n4. **Exploratory Data Analysis** - Descriptive statistics, distributions\n5. **Hypothesis Testing** - 5 statistical tests with assumptions checking\n6. **Regression Analysis** - 3 models with diagnostics and validation\n7. **Visualizations** - Publication-quality figures\n8. **Results Export** - Tables and plots for reporting\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# LIBRARY IMPORTS\n# ============================================================================\n\n# Data manipulation\nimport pandas as pd\nimport numpy as np\nimport warnings\n\n# Statistical analysis\nfrom scipy import stats\nfrom scipy.stats import chi2_contingency, normaltest, levene, shapiro\nimport statsmodels.api as sm\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,\n                              accuracy_score, precision_score, recall_score,\n                              f1_score, confusion_matrix, classification_report,\n                              roc_curve, roc_auc_score)\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# System\nimport os\nfrom datetime import datetime\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\n# Set random seed for reproducibility\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# Pandas display settings\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.precision', 4)\n\n# Matplotlib settings\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['savefig.dpi'] = 300\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 11\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 9\nplt.rcParams['ytick.labelsize'] = 9\nplt.rcParams['legend.fontsize'] = 9\n\n# Create output directory with timestamp\nTIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\nOUTPUT_DIR = 'analysis_results'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(\"=\"*80)\nprint(\"‚úÖ ENVIRONMENT SETUP COMPLETE\")\nprint(\"=\"*80)\nprint(f\"Random Seed: {RANDOM_STATE}\")\nprint(f\"Output Directory: {OUTPUT_DIR}/\")\nprint(f\"Timestamp: {TIMESTAMP}\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Load ‡πÅ‡∏•‡∏∞ Clean Data\n",
    "\n",
    "### 2.1 Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# HELPER FUNCTIONS\n# ============================================================================\n\ndef load_data(file_path):\n    \"\"\"\n    Load CSV data with error handling and validation.\n    \n    Parameters:\n    -----------\n    file_path : str\n        Path to CSV file\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Loaded dataframe\n    \"\"\"\n    try:\n        df = pd.read_csv(file_path)\n        print(f\"‚úì Loaded {file_path}: {df.shape}\")\n        return df\n    except FileNotFoundError:\n        print(f\"‚úó Error: File not found - {file_path}\")\n        return None\n    except Exception as e:\n        print(f\"‚úó Error loading {file_path}: {str(e)}\")\n        return None\n\ndef validate_data(df, name):\n    \"\"\"\n    Validate dataset for missing values and duplicates.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Dataset to validate\n    name : str\n        Name of dataset for reporting\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"DATA QUALITY REPORT: {name}\")\n    print(f\"{'='*60}\")\n    print(f\"Shape: {df.shape}\")\n    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n    \n    # Check for missing values\n    missing = df.isnull().sum()\n    if missing.sum() > 0:\n        print(f\"\\n‚ö†Ô∏è  Missing Values:\")\n        missing_pct = (missing / len(df) * 100).round(2)\n        missing_df = pd.DataFrame({\n            'Count': missing[missing > 0],\n            'Percentage': missing_pct[missing > 0]\n        }).sort_values('Count', ascending=False)\n        print(missing_df)\n    else:\n        print(\"\\n‚úì No missing values\")\n    \n    # Check for duplicates\n    duplicates = df.duplicated().sum()\n    if duplicates > 0:\n        print(f\"\\n‚ö†Ô∏è  Duplicate rows: {duplicates} ({duplicates/len(df)*100:.2f}%)\")\n    else:\n        print(\"‚úì No duplicate rows\")\n    \n    print(f\"{'='*60}\\n\")\n\n# ============================================================================\n# LOAD RAW DATA\n# ============================================================================\n\nprint(\"üìä LOADING RAW DATASETS...\")\nprint(\"=\"*80)\n\nraces = load_data('/mnt/user-data/uploads/races.csv')\nresults = load_data('/mnt/user-data/uploads/results.csv')\ndrivers = load_data('/mnt/user-data/uploads/drivers.csv')\nconstructors = load_data('/mnt/user-data/uploads/constructors.csv')\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"DATASET OVERVIEW\")\nprint(\"=\"*80)\n\nif all(df is not None for df in [races, results, drivers, constructors]):\n    print(f\"Races:        {races.shape[0]:>6,} rows √ó {races.shape[1]:>2} columns\")\n    print(f\"Results:      {results.shape[0]:>6,} rows √ó {results.shape[1]:>2} columns\")\n    print(f\"Drivers:      {drivers.shape[0]:>6,} rows √ó {drivers.shape[1]:>2} columns\")\n    print(f\"Constructors: {constructors.shape[0]:>6,} rows √ó {constructors.shape[1]:>2} columns\")\n    print(\"=\"*80)\n    \n    # Validate each dataset\n    for df, name in [(races, 'Races'), (results, 'Results'), \n                     (drivers, 'Drivers'), (constructors, 'Constructors')]:\n        validate_data(df, name)\nelse:\n    print(\"‚úó Error: Could not load all datasets\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Merge ‡πÅ‡∏•‡∏∞ Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all datasets\n",
    "df = results.merge(races, on='raceId', how='left')\n",
    "df = df.merge(drivers, on='driverId', how='left')\n",
    "df = df.merge(constructors, on='constructorId', how='left')\n",
    "\n",
    "# Filter Modern Era (2010-2024)\n",
    "df = df[df['year'] >= 2010].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Modern Era Data (2010-2024): {df.shape[0]} records\")\n",
    "print(f\"   Years: {df['year'].min()} - {df['year'].max()}\")\n",
    "print(f\"   Unique Drivers: {df['driverId'].nunique()}\")\n",
    "print(f\"   Unique Constructors: {df['constructorId'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Cleaning ‡πÅ‡∏•‡∏∞ Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CLEAN NUMERIC FIELDS =====\n",
    "\n",
    "# Position - convert to numeric\n",
    "df['position_num'] = pd.to_numeric(df['position'], errors='coerce')\n",
    "\n",
    "# Grid position\n",
    "df['grid'] = pd.to_numeric(df['grid'], errors='coerce')\n",
    "\n",
    "# Points\n",
    "df['points'] = pd.to_numeric(df['points'], errors='coerce')\n",
    "\n",
    "# Fastest lap speed\n",
    "df['fastestLapSpeed'] = pd.to_numeric(df['fastestLapSpeed'], errors='coerce')\n",
    "\n",
    "# ===== CREATE DERIVED VARIABLES =====\n",
    "\n",
    "# Won race (1st place)\n",
    "df['won'] = (df['position_num'] == 1).astype(int)\n",
    "\n",
    "# Podium finish (Top 3)\n",
    "df['podium'] = (df['position_num'] <= 3).astype(int)\n",
    "\n",
    "# Points scored (Yes/No)\n",
    "df['points_scored'] = (df['points'] > 0).astype(int)\n",
    "\n",
    "# Pole position (Started 1st)\n",
    "df['pole_position'] = (df['grid'] == 1).astype(int)\n",
    "\n",
    "# Top 3 grid\n",
    "df['top3_grid'] = (df['grid'] <= 3).astype(int)\n",
    "\n",
    "# Position change (grid - final position)\n",
    "df['position_change'] = df['grid'] - df['position_num']\n",
    "\n",
    "# ===== CALCULATE DRIVER AGE =====\n",
    "\n",
    "# Convert dates to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "drivers['dob'] = pd.to_datetime(drivers['dob'])\n",
    "\n",
    "# Calculate age at race\n",
    "driver_dob = drivers.set_index('driverId')['dob']\n",
    "df['driver_dob'] = df['driverId'].map(driver_dob)\n",
    "df['age_at_race'] = (df['date'] - df['driver_dob']).dt.days / 365.25\n",
    "\n",
    "# ===== CREATE FULL NAMES =====\n",
    "\n",
    "df['driver_name'] = df['forename'] + ' ' + df['surname']\n",
    "df['constructor_name'] = df['name_y']  # Constructor name\n",
    "\n",
    "print(\"\\n‚úÖ Feature Engineering Complete!\")\n",
    "print(f\"   Total Features: {df.shape[1]}\")\n",
    "print(f\"   Key Variables: position_num, grid, points, won, podium, age_at_race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key columns\n",
    "columns_to_save = [\n",
    "    'raceId', 'driverId', 'constructorId', 'year', 'round', 'circuitId',\n",
    "    'grid', 'position_num', 'points', 'laps', 'milliseconds',\n",
    "    'driver_name', 'constructor_name',\n",
    "    'won', 'podium', 'points_scored', 'pole_position', 'top3_grid',\n",
    "    'position_change', 'age_at_race', 'fastestLapSpeed'\n",
    "]\n",
    "\n",
    "df_clean = df[columns_to_save].copy()\n",
    "df_clean.to_csv('f1_modern_cleaned.csv', index=False)\n",
    "\n",
    "print(f\"\\nüíæ Cleaned data saved: f1_modern_cleaned.csv\")\n",
    "print(f\"   Shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Descriptive Statistics\n",
    "\n",
    "### 3.1 Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns\n",
    "numeric_cols = ['points', 'grid', 'position_num', 'age_at_race']\n",
    "desc_stats = df[numeric_cols].describe()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(desc_stats)\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\nüìà Additional Statistics:\")\n",
    "for col in numeric_cols:\n",
    "    data = df[col].dropna()\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Mean: {data.mean():.2f}\")\n",
    "    print(f\"  Median: {data.median():.2f}\")\n",
    "    print(f\"  Mode: {data.mode().values[0] if len(data.mode()) > 0 else 'N/A'}\")\n",
    "    print(f\"  Std Dev: {data.std():.2f}\")\n",
    "    print(f\"  Range: [{data.min():.2f}, {data.max():.2f}]\")\n",
    "    print(f\"  IQR: {data.quantile(0.75) - data.quantile(0.25):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Top Performers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TOP DRIVERS =====\n",
    "top_drivers = df.groupby('driver_name').agg({\n",
    "    'points': 'sum',\n",
    "    'won': 'sum',\n",
    "    'podium': 'sum',\n",
    "    'raceId': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "top_drivers.columns = ['Driver', 'Total_Points', 'Wins', 'Podiums', 'Races']\n",
    "top_drivers['Avg_Points'] = top_drivers['Total_Points'] / top_drivers['Races']\n",
    "top_drivers = top_drivers.sort_values('Total_Points', ascending=False).head(10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ TOP 10 DRIVERS (2010-2024)\")\n",
    "print(\"=\"*80)\n",
    "print(top_drivers.to_string(index=False))\n",
    "\n",
    "# ===== TOP CONSTRUCTORS =====\n",
    "top_constructors = df.groupby('constructor_name').agg({\n",
    "    'points': 'sum',\n",
    "    'won': 'sum',\n",
    "    'podium': 'sum',\n",
    "    'raceId': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "top_constructors.columns = ['Constructor', 'Total_Points', 'Wins', 'Podiums', 'Races']\n",
    "top_constructors['Avg_Points'] = top_constructors['Total_Points'] / top_constructors['Races']\n",
    "top_constructors = top_constructors.sort_values('Total_Points', ascending=False).head(10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ TOP 10 CONSTRUCTORS (2010-2024)\")\n",
    "print(\"=\"*80)\n",
    "print(top_constructors.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Hypothesis Testing\n",
    "\n",
    "### 4.1 One-Sample Proportion Test (z-test)\n",
    "\n",
    "**Question:** Does pole position (starting 1st) give a win rate > 50%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# TEST 1: ONE-SAMPLE PROPORTION TEST\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä TEST 1: ONE-SAMPLE PROPORTION TEST (Pole Position Advantage)\")\nprint(\"=\"*80)\n\n# Filter: races where someone started from pole\npole_races = df[df['pole_position'] == 1].copy()\n\n# Calculate proportion of wins from pole\nn = len(pole_races)\nwins_from_pole = pole_races['won'].sum()\np_hat = wins_from_pole / n\n\nprint(f\"\\n{'Sample Statistics':<30}\")\nprint(f\"  n (races from pole):         {n}\")\nprint(f\"  Wins from pole:              {wins_from_pole}\")\nprint(f\"  Sample proportion (pÃÇ):       {p_hat:.4f}\")\n\n# Check sample size adequacy\nprint(f\"\\n{'Assumptions Check':<30}\")\np0 = 0.5\nexpected_success = n * p0\nexpected_failure = n * (1 - p0)\nprint(f\"  n √ó œÄ‚ÇÄ:                      {expected_success:.1f} {'‚úì' if expected_success >= 10 else '‚úó'}\")\nprint(f\"  n √ó (1-œÄ‚ÇÄ):                  {expected_failure:.1f} {'‚úì' if expected_failure >= 10 else '‚úó'}\")\n\nif expected_success >= 10 and expected_failure >= 10:\n    print(\"  ‚úì Sample size adequate for normal approximation\")\nelse:\n    print(\"  ‚úó WARNING: Sample size may be inadequate\")\n\n# Hypotheses\nprint(f\"\\n{'Hypotheses':<30}\")\nprint(f\"  H‚ÇÄ: œÄ = 0.5 (pole gives 50% win rate)\")\nprint(f\"  H‚ÇÅ: œÄ > 0.5 (pole gives >50% win rate)\")\nprint(f\"  Significance level (Œ±): 0.05\")\n\n# One-sample proportion z-test\nse = np.sqrt(p0 * (1 - p0) / n)\nz_stat = (p_hat - p0) / se\np_value = 1 - stats.norm.cdf(z_stat)\n\nprint(f\"\\n{'Test Statistics':<30}\")\nprint(f\"  z-statistic:                 {z_stat:.4f}\")\nprint(f\"  p-value (one-tailed):        {p_value:.4f}\")\n\n# 95% Confidence Interval\nci_lower = p_hat - 1.96 * np.sqrt(p_hat * (1 - p_hat) / n)\nci_upper = p_hat + 1.96 * np.sqrt(p_hat * (1 - p_hat) / n)\nprint(f\"  95% CI for œÄ:                [{ci_lower:.4f}, {ci_upper:.4f}]\")\n\n# Decision\nalpha = 0.05\nprint(f\"\\n{'Decision':<30}\")\nif p_value < alpha:\n    decision = \"‚úì Reject H‚ÇÄ\"\n    conclusion = \"Pole position gives significant advantage (>50% win rate)\"\nelse:\n    decision = \"‚úó Fail to reject H‚ÇÄ\"\n    conclusion = \"No significant evidence that pole position gives >50% win rate\"\n\nprint(f\"  {decision}\")\nprint(f\"\\n{'Interpretation':<30}\")\nprint(f\"  {conclusion}\")\nprint(f\"  Effect size: {abs(p_hat - p0):.4f} ({abs(p_hat - p0)*100:.1f}% difference from H‚ÇÄ)\")\n\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Independent t-test\n",
    "\n",
    "**Question:** Is there a difference in average points between Hamilton and Verstappen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# TEST 2: INDEPENDENT T-TEST (with Assumptions Check)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä TEST 2: INDEPENDENT T-TEST (Hamilton vs Verstappen)\")\nprint(\"=\"*80)\n\n# Filter data\nhamilton = df[df['driver_name'] == 'Lewis Hamilton']['points'].dropna()\nverstappen = df[df['driver_name'] == 'Max Verstappen']['points'].dropna()\n\nprint(f\"\\n{'Sample Statistics':<30}\")\nprint(f\"  Hamilton:   n={len(hamilton):<5} Œº={hamilton.mean():>6.2f}  œÉ={hamilton.std():>6.2f}\")\nprint(f\"  Verstappen: n={len(verstappen):<5} Œº={verstappen.mean():>6.2f}  œÉ={verstappen.std():>6.2f}\")\n\n# ============================================================================\n# ASSUMPTIONS CHECKING\n# ============================================================================\n\nprint(f\"\\n{'Assumptions Check':<30}\")\nprint(\"-\" * 60)\n\n# 1. Independence (assumed by study design)\nprint(\"1. Independence:              ‚úì (different drivers, separate races)\")\n\n# 2. Normality check (Shapiro-Wilk for n < 50, else Anderson-Darling)\nprint(\"\\n2. Normality:\")\nif len(hamilton) < 5000:  # Shapiro-Wilk limit\n    stat_h, p_h = shapiro(hamilton)\n    stat_v, p_v = shapiro(verstappen)\n    test_name = \"Shapiro-Wilk\"\nelse:\n    stat_h, p_h = normaltest(hamilton)\n    stat_v, p_v = normaltest(verstappen)\n    test_name = \"D'Agostino-Pearson\"\n\nprint(f\"   Hamilton ({test_name}):\")\nprint(f\"     p-value = {p_h:.4f}     {'‚úì Normal' if p_h > 0.05 else '‚ö†Ô∏è  Non-normal (but robust for large n)'}\")\nprint(f\"   Verstappen ({test_name}):\")\nprint(f\"     p-value = {p_v:.4f}     {'‚úì Normal' if p_v > 0.05 else '‚ö†Ô∏è  Non-normal (but robust for large n)'}\")\n\n# Note about Central Limit Theorem\nif len(hamilton) >= 30 and len(verstappen) >= 30:\n    print(\"   ‚úì Large samples (n ‚â• 30): CLT applies, t-test is robust\")\n\n# 3. Homogeneity of variance (Levene's test)\nprint(\"\\n3. Equal Variances (Levene's test):\")\nstat_levene, p_levene = levene(hamilton, verstappen)\nprint(f\"   F-statistic = {stat_levene:.4f}\")\nprint(f\"   p-value = {p_levene:.4f}\")\nif p_levene > 0.05:\n    print(\"   ‚úì Equal variances assumed\")\n    equal_var = True\nelse:\n    print(\"   ‚ö†Ô∏è  Unequal variances (will use Welch's t-test)\")\n    equal_var = False\n\n# ============================================================================\n# HYPOTHESIS TEST\n# ============================================================================\n\nprint(f\"\\n{'Hypotheses':<30}\")\nprint(f\"  H‚ÇÄ: Œº_Hamilton = Œº_Verstappen\")\nprint(f\"  H‚ÇÅ: Œº_Hamilton ‚â† Œº_Verstappen\")\nprint(f\"  Significance level (Œ±): 0.05\")\n\n# Independent t-test (with appropriate variance assumption)\nt_stat, p_value = stats.ttest_ind(hamilton, verstappen, equal_var=equal_var)\n\nprint(f\"\\n{'Test Statistics':<30}\")\nprint(f\"  t-statistic:                 {t_stat:.4f}\")\nprint(f\"  p-value (two-tailed):        {p_value:.4f}\")\nif equal_var:\n    df_t = len(hamilton) + len(verstappen) - 2\n    print(f\"  degrees of freedom:          {df_t}\")\nelse:\n    # Welch-Satterthwaite df approximation\n    s1_sq = hamilton.var()\n    s2_sq = verstappen.var()\n    n1, n2 = len(hamilton), len(verstappen)\n    df_t = ((s1_sq/n1 + s2_sq/n2)**2) / ((s1_sq/n1)**2/(n1-1) + (s2_sq/n2)**2/(n2-1))\n    print(f\"  degrees of freedom (Welch):  {df_t:.2f}\")\n\n# Effect size (Cohen's d)\npooled_std = np.sqrt(((len(hamilton)-1)*hamilton.var() + \n                       (len(verstappen)-1)*verstappen.var()) / \n                      (len(hamilton) + len(verstappen) - 2))\ncohens_d = (hamilton.mean() - verstappen.mean()) / pooled_std\n\nprint(f\"  Cohen's d:                   {cohens_d:.4f}\", end=\"\")\nif abs(cohens_d) < 0.2:\n    effect_interp = \" (negligible effect)\"\nelif abs(cohens_d) < 0.5:\n    effect_interp = \" (small effect)\"\nelif abs(cohens_d) < 0.8:\n    effect_interp = \" (medium effect)\"\nelse:\n    effect_interp = \" (large effect)\"\nprint(effect_interp)\n\n# 95% CI for difference in means\nse_diff = pooled_std * np.sqrt(1/len(hamilton) + 1/len(verstappen))\ndiff_mean = hamilton.mean() - verstappen.mean()\nci_lower = diff_mean - 1.96 * se_diff\nci_upper = diff_mean + 1.96 * se_diff\nprint(f\"  95% CI for difference:       [{ci_lower:.4f}, {ci_upper:.4f}]\")\n\n# Decision\nalpha = 0.05\nprint(f\"\\n{'Decision':<30}\")\nif p_value < alpha:\n    decision = \"‚úì Reject H‚ÇÄ\"\n    conclusion = \"There IS a significant difference between Hamilton and Verstappen\"\nelse:\n    decision = \"‚úó Fail to reject H‚ÇÄ\"\n    conclusion = \"No significant difference between Hamilton and Verstappen\"\n\nprint(f\"  {decision}\")\nprint(f\"\\n{'Interpretation':<30}\")\nprint(f\"  {conclusion}\")\nprint(f\"  Difference in means: {diff_mean:.2f} points\")\n\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 One-Way ANOVA\n",
    "\n",
    "**Question:** Is there a difference in average points among top 3 constructors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# TEST 3: ONE-WAY ANOVA (with Assumptions Check)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä TEST 3: ONE-WAY ANOVA (Top 3 Constructors)\")\nprint(\"=\"*80)\n\n# Filter top 3 constructors\ntop3_teams = ['Mercedes', 'Red Bull', 'Ferrari']\ndf_top3 = df[df['constructor_name'].isin(top3_teams)].copy()\n\n# Separate groups\nmercedes = df_top3[df_top3['constructor_name'] == 'Mercedes']['points'].dropna()\nredbull = df_top3[df_top3['constructor_name'] == 'Red Bull']['points'].dropna()\nferrari = df_top3[df_top3['constructor_name'] == 'Ferrari']['points'].dropna()\n\nprint(f\"\\n{'Group Statistics':<30}\")\nprint(f\"  Mercedes:  n={len(mercedes):<5} Œº={mercedes.mean():>6.2f}  œÉ={mercedes.std():>6.2f}\")\nprint(f\"  Red Bull:  n={len(redbull):<5} Œº={redbull.mean():>6.2f}  œÉ={redbull.std():>6.2f}\")\nprint(f\"  Ferrari:   n={len(ferrari):<5} Œº={ferrari.mean():>6.2f}  œÉ={ferrari.std():>6.2f}\")\n\n# ============================================================================\n# ASSUMPTIONS CHECKING\n# ============================================================================\n\nprint(f\"\\n{'Assumptions Check':<30}\")\nprint(\"-\" * 60)\n\n# 1. Independence\nprint(\"1. Independence:              ‚úì (independent race results)\")\n\n# 2. Normality (for each group)\nprint(\"\\n2. Normality (per group):\")\nfor name, group in [('Mercedes', mercedes), ('Red Bull', redbull), ('Ferrari', ferrari)]:\n    if len(group) < 5000:\n        stat, p = shapiro(group)\n        test_name = \"Shapiro-Wilk\"\n    else:\n        stat, p = normaltest(group)\n        test_name = \"D'Agostino-Pearson\"\n    \n    status = '‚úì Normal' if p > 0.05 else '‚ö†Ô∏è  Non-normal (ANOVA robust for large n)'\n    print(f\"   {name:12} ({test_name}): p={p:.4f}  {status}\")\n\nif all(len(g) >= 30 for g in [mercedes, redbull, ferrari]):\n    print(\"   ‚úì All groups have n ‚â• 30: CLT applies, ANOVA is robust\")\n\n# 3. Homogeneity of variance (Levene's test)\nprint(\"\\n3. Homogeneity of Variance (Levene's test):\")\nstat_levene, p_levene = levene(mercedes, redbull, ferrari)\nprint(f\"   F-statistic = {stat_levene:.4f}\")\nprint(f\"   p-value = {p_levene:.4f}\")\nif p_levene > 0.05:\n    print(\"   ‚úì Equal variances across groups\")\nelse:\n    print(\"   ‚ö†Ô∏è  Unequal variances (consider Welch's ANOVA)\")\n\n# ============================================================================\n# HYPOTHESIS TEST\n# ============================================================================\n\nprint(f\"\\n{'Hypotheses':<30}\")\nprint(f\"  H‚ÇÄ: Œº_Mercedes = Œº_RedBull = Œº_Ferrari\")\nprint(f\"  H‚ÇÅ: At least one mean is different\")\nprint(f\"  Significance level (Œ±): 0.05\")\n\n# One-way ANOVA\nf_stat, p_value = stats.f_oneway(mercedes, redbull, ferrari)\n\n# Calculate degrees of freedom\nk = 3  # number of groups\nn_total = len(mercedes) + len(redbull) + len(ferrari)\ndf_between = k - 1\ndf_within = n_total - k\n\n# Calculate eta-squared (effect size)\ngrand_mean = df_top3['points'].mean()\nss_between = sum(len(g) * (g.mean() - grand_mean)**2 for g in [mercedes, redbull, ferrari])\nss_within = sum((g - g.mean()).sum()**2 for g in [mercedes, redbull, ferrari])\nss_total = ss_between + ss_within\neta_squared = ss_between / ss_total\n\nprint(f\"\\n{'Test Statistics':<30}\")\nprint(f\"  F-statistic:                 {f_stat:.4f}\")\nprint(f\"  p-value:                     {p_value:.10f}\")\nprint(f\"  df (between groups):         {df_between}\")\nprint(f\"  df (within groups):          {df_within}\")\nprint(f\"  Œ∑¬≤ (eta-squared):            {eta_squared:.4f}\", end=\"\")\n\nif eta_squared < 0.01:\n    effect_interp = \" (negligible)\"\nelif eta_squared < 0.06:\n    effect_interp = \" (small)\"\nelif eta_squared < 0.14:\n    effect_interp = \" (medium)\"\nelse:\n    effect_interp = \" (large)\"\nprint(effect_interp)\n\n# Decision\nalpha = 0.05\nprint(f\"\\n{'Decision':<30}\")\nif p_value < alpha:\n    decision = \"‚úì Reject H‚ÇÄ\"\n    conclusion = \"There IS a significant difference among constructors\"\n    do_posthoc = True\nelse:\n    decision = \"‚úó Fail to reject H‚ÇÄ\"\n    conclusion = \"No significant difference among constructors\"\n    do_posthoc = False\n\nprint(f\"  {decision}\")\nprint(f\"\\n{'Interpretation':<30}\")\nprint(f\"  {conclusion}\")\n\n# ============================================================================\n# POST-HOC TEST (if significant)\n# ============================================================================\n\nif do_posthoc:\n    print(f\"\\n{'='*80}\")\n    print(\"üìä POST-HOC: Tukey HSD Test (pairwise comparisons)\")\n    print(\"=\"*80)\n    \n    tukey_result = pairwise_tukeyhsd(\n        df_top3['points'].dropna(),\n        df_top3['constructor_name'],\n        alpha=0.05\n    )\n    print(tukey_result)\n    \n    # Interpret Tukey results\n    print(\"\\nInterpretation:\")\n    print(\"  'reject=True' means the pair has significantly different means\")\n    print(\"  'meandiff' shows the difference in average points per race\")\n\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Chi-Square Test\n",
    "\n",
    "**Question:** Is there an association between Top 3 grid and Podium finish?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä TEST 4: CHI-SQUARE TEST (Grid Position vs Podium)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create contingency table\n",
    "df_valid = df[df['grid'].notna() & df['position_num'].notna()].copy()\n",
    "contingency = pd.crosstab(df_valid['top3_grid'], df_valid['podium'])\n",
    "\n",
    "print(f\"\\nContingency Table:\")\n",
    "print(contingency)\n",
    "print(f\"\\n(0 = No, 1 = Yes)\")\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(f\"\\nTest Statistics:\")\n",
    "print(f\"  œá¬≤ statistic: {chi2:.4f}\")\n",
    "print(f\"  p-value: {p_value:.10f}\")\n",
    "print(f\"  degrees of freedom: {dof}\")\n",
    "\n",
    "# Cram√©r's V (effect size)\n",
    "n = contingency.sum().sum()\n",
    "cramers_v = np.sqrt(chi2 / (n * min(contingency.shape[0]-1, contingency.shape[1]-1)))\n",
    "print(f\"  Cram√©r's V: {cramers_v:.4f}\", end=\"\")\n",
    "if cramers_v < 0.1:\n",
    "    print(\" (negligible)\")\n",
    "elif cramers_v < 0.3:\n",
    "    print(\" (small)\")\n",
    "elif cramers_v < 0.5:\n",
    "    print(\" (medium)\")\n",
    "else:\n",
    "    print(\" (large)\")\n",
    "\n",
    "# Hypothesis\n",
    "print(f\"\\nHypotheses:\")\n",
    "print(f\"  H‚ÇÄ: Grid position and Podium are independent\")\n",
    "print(f\"  H‚ÇÅ: Grid position and Podium are associated\")\n",
    "\n",
    "# Decision\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    decision = \"Reject H‚ÇÄ\"\n",
    "    conclusion = \"There IS a significant association between Top 3 grid and Podium\"\n",
    "else:\n",
    "    decision = \"Fail to reject H‚ÇÄ\"\n",
    "    conclusion = \"No significant association\"\n",
    "\n",
    "print(f\"\\nDecision (Œ± = {alpha}):\")\n",
    "print(f\"  {decision}\")\n",
    "print(f\"  Conclusion: {conclusion}\")\n",
    "\n",
    "# Show proportions\n",
    "print(f\"\\nüìä Podium Rate by Grid Position:\")\n",
    "top3_podium_rate = df_valid[df_valid['top3_grid']==1]['podium'].mean()\n",
    "other_podium_rate = df_valid[df_valid['top3_grid']==0]['podium'].mean()\n",
    "print(f\"  Top 3 Grid ‚Üí Podium: {top3_podium_rate:.1%}\")\n",
    "print(f\"  Other Grid ‚Üí Podium: {other_podium_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Correlation Test\n",
    "\n",
    "**Question:** What is the correlation between Grid Position and Final Position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä TEST 5: PEARSON CORRELATION (Grid vs Final Position)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter valid data\n",
    "df_corr = df[(df['grid'].notna()) & (df['position_num'].notna())].copy()\n",
    "\n",
    "# Calculate correlation\n",
    "r, p_value = stats.pearsonr(df_corr['grid'], df_corr['position_num'])\n",
    "\n",
    "print(f\"\\nSample Statistics:\")\n",
    "print(f\"  n: {len(df_corr)}\")\n",
    "print(f\"  Correlation coefficient (r): {r:.4f}\")\n",
    "print(f\"  R¬≤ (coefficient of determination): {r**2:.4f}\")\n",
    "\n",
    "# Hypothesis\n",
    "print(f\"\\nHypotheses:\")\n",
    "print(f\"  H‚ÇÄ: œÅ = 0 (no correlation)\")\n",
    "print(f\"  H‚ÇÅ: œÅ ‚â† 0 (correlation exists)\")\n",
    "\n",
    "print(f\"\\nTest Statistics:\")\n",
    "print(f\"  r: {r:.4f}\")\n",
    "print(f\"  p-value: {p_value:.10f}\")\n",
    "\n",
    "# Interpretation\n",
    "if abs(r) < 0.3:\n",
    "    strength = \"weak\"\n",
    "elif abs(r) < 0.7:\n",
    "    strength = \"moderate\"\n",
    "else:\n",
    "    strength = \"strong\"\n",
    "\n",
    "direction = \"positive\" if r > 0 else \"negative\"\n",
    "\n",
    "print(f\"  Interpretation: {strength} {direction} correlation\")\n",
    "\n",
    "# Decision\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    decision = \"Reject H‚ÇÄ\"\n",
    "    conclusion = f\"There IS a significant {strength} {direction} correlation\"\n",
    "else:\n",
    "    decision = \"Fail to reject H‚ÇÄ\"\n",
    "    conclusion = \"No significant correlation\"\n",
    "\n",
    "print(f\"\\nDecision (Œ± = {alpha}):\")\n",
    "print(f\"  {decision}\")\n",
    "print(f\"  Conclusion: {conclusion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Regression Analysis\n",
    "\n",
    "### 5.1 Simple Linear Regression\n",
    "\n",
    "**Model:** Final Position = Œ≤‚ÇÄ + Œ≤‚ÇÅ(Grid Position) + Œµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# SIMPLE LINEAR REGRESSION (with Diagnostics)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä SIMPLE LINEAR REGRESSION\")\nprint(\"=\"*80)\n\n# Prepare data\ndf_reg = df[(df['grid'].notna()) & (df['position_num'].notna())].copy()\nX = df_reg[['grid']]\ny = df_reg['position_num']\n\nprint(f\"\\n{'Model Specification':<30}\")\nprint(f\"  Dependent variable:          Final Position\")\nprint(f\"  Independent variable:        Grid Position\")\nprint(f\"  Sample size:                 {len(df_reg):,}\")\n\n# ============================================================================\n# TRAIN-TEST SPLIT (for validation)\n# ============================================================================\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=RANDOM_STATE\n)\n\nprint(f\"  Training set:                {len(X_train):,} ({len(X_train)/len(df_reg)*100:.1f}%)\")\nprint(f\"  Test set:                    {len(X_test):,} ({len(X_test)/len(df_reg)*100:.1f}%)\")\n\n# ============================================================================\n# FIT MODEL\n# ============================================================================\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\nprint(f\"\\n{'Model Coefficients':<30}\")\nprint(f\"  Œ≤‚ÇÄ (Intercept):              {model.intercept_:.4f}\")\nprint(f\"  Œ≤‚ÇÅ (Grid):                   {model.coef_[0]:.4f}\")\nprint(f\"\\n  Equation: Position = {model.intercept_:.2f} + {model.coef_[0]:.2f} √ó Grid\")\n\n# ============================================================================\n# MODEL PERFORMANCE\n# ============================================================================\n\nprint(f\"\\n{'Model Performance':<30}\")\nprint(\"-\" * 60)\n\n# Training metrics\nr2_train = r2_score(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nmae_train = mean_absolute_error(y_train, y_pred_train)\n\nprint(f\"Training Set:\")\nprint(f\"  R¬≤:                          {r2_train:.4f}\")\nprint(f\"  RMSE:                        {rmse_train:.4f}\")\nprint(f\"  MAE:                         {mae_train:.4f}\")\n\n# Test metrics\nr2_test = r2_score(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nmae_test = mean_absolute_error(y_test, y_pred_test)\n\nprint(f\"\\nTest Set:\")\nprint(f\"  R¬≤:                          {r2_test:.4f}\")\nprint(f\"  RMSE:                        {rmse_test:.4f}\")\nprint(f\"  MAE:                         {mae_test:.4f}\")\n\n# Check for overfitting\nr2_diff = abs(r2_train - r2_test)\nif r2_diff < 0.05:\n    print(f\"\\n  ‚úì Model generalizes well (R¬≤ diff = {r2_diff:.4f})\")\nelif r2_diff < 0.10:\n    print(f\"\\n  ‚ö†Ô∏è  Slight overfitting (R¬≤ diff = {r2_diff:.4f})\")\nelse:\n    print(f\"\\n  ‚úó Overfitting detected (R¬≤ diff = {r2_diff:.4f})\")\n\n# ============================================================================\n# CROSS-VALIDATION\n# ============================================================================\n\nprint(f\"\\n{'Cross-Validation (5-fold)':<30}\")\ncv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\nprint(f\"  Mean R¬≤:                     {cv_scores.mean():.4f}\")\nprint(f\"  Std R¬≤:                      {cv_scores.std():.4f}\")\nprint(f\"  Min R¬≤:                      {cv_scores.min():.4f}\")\nprint(f\"  Max R¬≤:                      {cv_scores.max():.4f}\")\n\n# ============================================================================\n# STATISTICAL SIGNIFICANCE\n# ============================================================================\n\nprint(f\"\\n{'Statistical Tests':<30}\")\nprint(\"-\" * 60)\n\nX_with_const = sm.add_constant(X_train)\nmodel_sm = sm.OLS(y_train, X_with_const).fit()\n\nprint(\"\\nOLS Regression Results:\")\nprint(model_sm.summary().tables[1])\n\nprint(f\"\\n{'Interpretation':<30}\")\nprint(f\"  For each position back on grid, final position worsens by\")\nprint(f\"  {model.coef_[0]:.3f} positions on average (p < 0.001)\")\nprint(f\"  Model explains {r2_test*100:.1f}% of variance in final position\")\n\n# ============================================================================\n# REGRESSION DIAGNOSTICS\n# ============================================================================\n\nprint(f\"\\n{'Regression Diagnostics':<30}\")\nprint(\"-\" * 60)\n\nresiduals = y_train - y_pred_train\n\n# 1. Linearity (residuals should have no pattern)\nfrom scipy.stats import pearsonr\ncorr_res_pred, p_res = pearsonr(y_pred_train, residuals)\nprint(f\"1. Linearity:\")\nprint(f\"   Residuals vs Fitted corr:   {corr_res_pred:.4f} (p={p_res:.4f})\")\nif abs(corr_res_pred) < 0.1:\n    print(f\"   ‚úì Linear relationship assumption met\")\nelse:\n    print(f\"   ‚ö†Ô∏è  Non-linear pattern detected\")\n\n# 2. Homoscedasticity (constant variance)\nprint(f\"\\n2. Homoscedasticity:\")\n# Split into low/high predictions\nmedian_pred = np.median(y_pred_train)\nlow_residuals = residuals[y_pred_train <= median_pred]\nhigh_residuals = residuals[y_pred_train > median_pred]\nstat_levene, p_levene = levene(low_residuals, high_residuals)\nprint(f\"   Levene's test p-value:      {p_levene:.4f}\")\nif p_levene > 0.05:\n    print(f\"   ‚úì Constant variance assumption met\")\nelse:\n    print(f\"   ‚ö†Ô∏è  Heteroscedasticity detected\")\n\n# 3. Normality of residuals\nprint(f\"\\n3. Normality of Residuals:\")\nif len(residuals) < 5000:\n    stat_norm, p_norm = shapiro(residuals)\n    test_name = \"Shapiro-Wilk\"\nelse:\n    stat_norm, p_norm = normaltest(residuals)\n    test_name = \"D'Agostino-Pearson\"\nprint(f\"   {test_name} p-value:    {p_norm:.4f}\")\nif p_norm > 0.05:\n    print(f\"   ‚úì Residuals are normally distributed\")\nelse:\n    print(f\"   ‚ö†Ô∏è  Residuals not normal (but OK for large sample)\")\n\n# 4. Independence (Durbin-Watson)\nfrom statsmodels.stats.stattools import durbin_watson\ndw_stat = durbin_watson(residuals)\nprint(f\"\\n4. Independence:\")\nprint(f\"   Durbin-Watson statistic:    {dw_stat:.4f}\")\nif 1.5 < dw_stat < 2.5:\n    print(f\"   ‚úì No autocorrelation detected\")\nelse:\n    print(f\"   ‚ö†Ô∏è  Possible autocorrelation\")\n\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Multiple Linear Regression\n",
    "\n",
    "**Model:** Points = Œ≤‚ÇÄ + Œ≤‚ÇÅ(Grid) + Œ≤‚ÇÇ(Mercedes) + Œ≤‚ÇÉ(Red Bull) + Œµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# MULTIPLE LINEAR REGRESSION (with VIF and Diagnostics)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä MULTIPLE LINEAR REGRESSION\")\nprint(\"=\"*80)\n\n# Prepare data (top 3 constructors only)\ndf_multi = df[df['constructor_name'].isin(['Mercedes', 'Red Bull', 'Ferrari'])].copy()\ndf_multi = df_multi[(df_multi['grid'].notna()) & (df_multi['points'].notna())]\n\n# Create dummy variables\ndf_multi['is_mercedes'] = (df_multi['constructor_name'] == 'Mercedes').astype(int)\ndf_multi['is_redbull'] = (df_multi['constructor_name'] == 'Red Bull').astype(int)\n# Ferrari is reference category (both dummies = 0)\n\n# Features and target\nX = df_multi[['grid', 'is_mercedes', 'is_redbull']]\ny = df_multi['points']\n\nprint(f\"\\n{'Model Specification':<30}\")\nprint(f\"  Dependent variable:          Points\")\nprint(f\"  Independent variables:\")\nprint(f\"    - Grid Position (continuous)\")\nprint(f\"    - Mercedes (dummy: 1=yes, 0=no)\")\nprint(f\"    - Red Bull (dummy: 1=yes, 0=no)\")\nprint(f\"    - Reference: Ferrari\")\nprint(f\"  Sample size:                 {len(df_multi):,}\")\n\n# ============================================================================\n# TRAIN-TEST SPLIT\n# ============================================================================\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=RANDOM_STATE\n)\n\nprint(f\"  Training set:                {len(X_train):,} ({len(X_train)/len(df_multi)*100:.1f}%)\")\nprint(f\"  Test set:                    {len(X_test):,} ({len(X_test)/len(df_multi)*100:.1f}%)\")\n\n# ============================================================================\n# FIT MODEL\n# ============================================================================\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\nprint(f\"\\n{'Model Coefficients':<30}\")\nprint(f\"  Œ≤‚ÇÄ (Intercept):              {model.intercept_:.4f}\")\nprint(f\"  Œ≤‚ÇÅ (Grid):                   {model.coef_[0]:.4f}\")\nprint(f\"  Œ≤‚ÇÇ (Mercedes):               {model.coef_[1]:.4f}\")\nprint(f\"  Œ≤‚ÇÉ (Red Bull):               {model.coef_[2]:.4f}\")\n\nprint(f\"\\n  Equation:\")\nprint(f\"  Points = {model.intercept_:.2f} + {model.coef_[0]:.2f}√óGrid\")\nprint(f\"           + {model.coef_[1]:.2f}√óMercedes + {model.coef_[2]:.2f}√óRedBull\")\n\n# ============================================================================\n# MODEL PERFORMANCE\n# ============================================================================\n\nprint(f\"\\n{'Model Performance':<30}\")\nprint(\"-\" * 60)\n\n# Training metrics\nr2_train = r2_score(y_train, y_pred_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nmae_train = mean_absolute_error(y_train, y_pred_train)\n\nprint(f\"Training Set:\")\nprint(f\"  R¬≤:                          {r2_train:.4f}\")\nprint(f\"  Adjusted R¬≤:                 {1 - (1-r2_train)*(len(X_train)-1)/(len(X_train)-X_train.shape[1]-1):.4f}\")\nprint(f\"  RMSE:                        {rmse_train:.4f}\")\nprint(f\"  MAE:                         {mae_train:.4f}\")\n\n# Test metrics\nr2_test = r2_score(y_test, y_pred_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\nmae_test = mean_absolute_error(y_test, y_pred_test)\n\nprint(f\"\\nTest Set:\")\nprint(f\"  R¬≤:                          {r2_test:.4f}\")\nprint(f\"  RMSE:                        {rmse_test:.4f}\")\nprint(f\"  MAE:                         {mae_test:.4f}\")\n\n# Check for overfitting\nr2_diff = abs(r2_train - r2_test)\nif r2_diff < 0.05:\n    print(f\"\\n  ‚úì Model generalizes well (R¬≤ diff = {r2_diff:.4f})\")\nelse:\n    print(f\"\\n  ‚ö†Ô∏è  Possible overfitting (R¬≤ diff = {r2_diff:.4f})\")\n\n# ============================================================================\n# MULTICOLLINEARITY CHECK (VIF)\n# ============================================================================\n\nprint(f\"\\n{'Multicollinearity Check (VIF)':<30}\")\nprint(\"-\" * 60)\n\nX_with_const = sm.add_constant(X_train)\nvif_data = pd.DataFrame()\nvif_data[\"Variable\"] = X_train.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n\nprint(vif_data.to_string(index=False))\nprint(\"\\nInterpretation:\")\nprint(\"  VIF < 5:  No multicollinearity ‚úì\")\nprint(\"  VIF 5-10: Moderate multicollinearity ‚ö†Ô∏è\")\nprint(\"  VIF > 10: High multicollinearity ‚úó\")\n\nif vif_data['VIF'].max() < 5:\n    print(f\"\\n  ‚úì No multicollinearity issues detected\")\nelif vif_data['VIF'].max() < 10:\n    print(f\"\\n  ‚ö†Ô∏è  Moderate multicollinearity detected\")\nelse:\n    print(f\"\\n  ‚úó High multicollinearity detected\")\n\n# ============================================================================\n# STATISTICAL SIGNIFICANCE\n# ============================================================================\n\nprint(f\"\\n{'Statistical Tests':<30}\")\nprint(\"-\" * 60)\n\nmodel_sm = sm.OLS(y_train, X_with_const).fit()\n\nprint(\"\\nOLS Regression Results:\")\nprint(model_sm.summary().tables[1])\n\nprint(f\"\\nModel Summary:\")\nprint(f\"  F-statistic:                 {model_sm.fvalue:.4f}\")\nprint(f\"  Prob (F-statistic):          {model_sm.f_pvalue:.10f}\")\nprint(f\"  AIC:                         {model_sm.aic:.2f}\")\nprint(f\"  BIC:                         {model_sm.bic:.2f}\")\n\nprint(f\"\\n{'Interpretation':<30}\")\nprint(f\"  - Each grid position back reduces points by {abs(model.coef_[0]):.2f}\")\nprint(f\"  - Mercedes gets {model.coef_[1]:.2f} more points than Ferrari\")\nprint(f\"  - Red Bull gets {model.coef_[2]:.2f} more points than Ferrari\")\nprint(f\"  - Model explains {r2_test*100:.1f}% of variance in points\")\n\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Logistic Regression\n",
    "\n",
    "**Model:** P(Podium) = logit‚Åª¬π(Œ≤‚ÇÄ + Œ≤‚ÇÅ(Grid Position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# LOGISTIC REGRESSION (with ROC Curve and Classification Metrics)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä LOGISTIC REGRESSION\")\nprint(\"=\"*80)\n\n# Prepare data\ndf_logit = df[(df['grid'].notna()) & (df['podium'].notna())].copy()\nX = df_logit[['grid']]\ny = df_logit['podium']\n\nprint(f\"\\n{'Model Specification':<30}\")\nprint(f\"  Dependent variable:          Podium (binary: 0=No, 1=Yes)\")\nprint(f\"  Independent variable:        Grid Position\")\nprint(f\"  Sample size:                 {len(df_logit):,}\")\nprint(f\"  Class distribution:\")\nprint(f\"    - No Podium (0):           {(y==0).sum():,} ({(y==0).sum()/len(y)*100:.1f}%)\")\nprint(f\"    - Podium (1):              {(y==1).sum():,} ({(y==1).sum()/len(y)*100:.1f}%)\")\n\n# Check class balance\nclass_ratio = (y==1).sum() / (y==0).sum()\nif 0.2 < class_ratio < 5:\n    print(f\"  ‚úì Classes reasonably balanced (ratio: {class_ratio:.2f})\")\nelse:\n    print(f\"  ‚ö†Ô∏è  Class imbalance detected (ratio: {class_ratio:.2f})\")\n\n# ============================================================================\n# TRAIN-TEST SPLIT (stratified)\n# ============================================================================\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n)\n\nprint(f\"\\n{'Train-Test Split (Stratified)':<30}\")\nprint(f\"  Training set:                {len(X_train):,} ({len(X_train)/len(df_logit)*100:.1f}%)\")\nprint(f\"    - Podium rate:             {y_train.mean()*100:.1f}%\")\nprint(f\"  Test set:                    {len(X_test):,} ({len(X_test)/len(df_logit)*100:.1f}%)\")\nprint(f\"    - Podium rate:             {y_test.mean()*100:.1f}%\")\n\n# ============================================================================\n# FIT MODEL\n# ============================================================================\n\nmodel = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\ny_pred_proba_train = model.predict_proba(X_train)[:, 1]\ny_pred_proba_test = model.predict_proba(X_test)[:, 1]\n\nprint(f\"\\n{'Model Coefficients':<30}\")\nprint(f\"  Œ≤‚ÇÄ (Intercept):              {model.intercept_[0]:.4f}\")\nprint(f\"  Œ≤‚ÇÅ (Grid):                   {model.coef_[0][0]:.4f}\")\n\n# Calculate odds ratio\nodds_ratio = np.exp(model.coef_[0][0])\nprint(f\"  Odds Ratio:                  {odds_ratio:.4f}\")\n\nprint(f\"\\n  Equation:\")\nprint(f\"  logit(P(Podium)) = {model.intercept_[0]:.2f} + {model.coef_[0][0]:.2f} √ó Grid\")\n\n# ============================================================================\n# MODEL PERFORMANCE\n# ============================================================================\n\nprint(f\"\\n{'Classification Metrics':<30}\")\nprint(\"-\" * 60)\n\n# Training metrics\nacc_train = accuracy_score(y_train, y_pred_train)\nprec_train = precision_score(y_train, y_pred_train, zero_division=0)\nrec_train = recall_score(y_train, y_pred_train, zero_division=0)\nf1_train = f1_score(y_train, y_pred_train, zero_division=0)\nauc_train = roc_auc_score(y_train, y_pred_proba_train)\n\nprint(f\"Training Set:\")\nprint(f\"  Accuracy:                    {acc_train:.4f} ({acc_train*100:.2f}%)\")\nprint(f\"  Precision:                   {prec_train:.4f}\")\nprint(f\"  Recall:                      {rec_train:.4f}\")\nprint(f\"  F1-Score:                    {f1_train:.4f}\")\nprint(f\"  AUC-ROC:                     {auc_train:.4f}\")\n\n# Test metrics\nacc_test = accuracy_score(y_test, y_pred_test)\nprec_test = precision_score(y_test, y_pred_test, zero_division=0)\nrec_test = recall_score(y_test, y_pred_test, zero_division=0)\nf1_test = f1_score(y_test, y_pred_test, zero_division=0)\nauc_test = roc_auc_score(y_test, y_pred_proba_test)\n\nprint(f\"\\nTest Set:\")\nprint(f\"  Accuracy:                    {acc_test:.4f} ({acc_test*100:.2f}%)\")\nprint(f\"  Precision:                   {prec_test:.4f}\")\nprint(f\"  Recall:                      {rec_test:.4f}\")\nprint(f\"  F1-Score:                    {f1_test:.4f}\")\nprint(f\"  AUC-ROC:                     {auc_test:.4f}\")\n\n# Check generalization\nacc_diff = abs(acc_train - acc_test)\nif acc_diff < 0.05:\n    print(f\"\\n  ‚úì Model generalizes well (accuracy diff = {acc_diff:.4f})\")\nelse:\n    print(f\"\\n  ‚ö†Ô∏è  Possible overfitting (accuracy diff = {acc_diff:.4f})\")\n\n# ============================================================================\n# CONFUSION MATRIX\n# ============================================================================\n\nprint(f\"\\n{'Confusion Matrix (Test Set)':<30}\")\nprint(\"-\" * 60)\n\ncm = confusion_matrix(y_test, y_pred_test)\ntn, fp, fn, tp = cm.ravel()\n\nprint(f\"\\n                 Predicted\")\nprint(f\"               No    Yes\")\nprint(f\"Actual  No    {tn:5d} {fp:5d}  (Specificity: {tn/(tn+fp):.3f})\")\nprint(f\"        Yes   {fn:5d} {tp:5d}  (Sensitivity: {tp/(tp+fn):.3f})\")\n\n# Additional metrics\nspecificity = tn / (tn + fp) if (tn + fp) > 0 else 0\nnpv = tn / (tn + fn) if (tn + fn) > 0 else 0\n\nprint(f\"\\nAdditional Metrics:\")\nprint(f\"  True Negative Rate:          {specificity:.4f}\")\nprint(f\"  False Positive Rate:         {fp/(fp+tn):.4f}\")\nprint(f\"  Negative Predictive Value:   {npv:.4f}\")\n\n# ============================================================================\n# INTERPRETATION\n# ============================================================================\n\nprint(f\"\\n{'Interpretation':<30}\")\nprint(\"-\" * 60)\nprint(f\"  - Each position back on grid multiplies podium odds by {odds_ratio:.3f}\")\nprint(f\"  - Or equivalently, reduces log-odds by {abs(model.coef_[0][0]):.3f}\")\nprint(f\"  - Model achieves {auc_test:.1%} AUC, indicating {'excellent' if auc_test > 0.9 else 'good' if auc_test > 0.8 else 'fair'} discrimination\")\nprint(f\"  - Starting from pole (grid=1) gives ~{model.predict_proba([[1]])[0][1]*100:.1f}% podium probability\")\nprint(f\"  - Starting from 10th gives ~{model.predict_proba([[10]])[0][1]*100:.1f}% podium probability\")\n\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Visualizations\n",
    "\n",
    "### 6.1 Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating Figure 1: Distributions...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Distribution of Key Variables (2010-2024)', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "\n",
    "# Points distribution\n",
    "axes[0,0].hist(df['points'].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0,0].axvline(df['points'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"points\"].mean():.2f}')\n",
    "axes[0,0].axvline(df['points'].median(), color='blue', linestyle='--', label=f'Median: {df[\"points\"].median():.2f}')\n",
    "axes[0,0].set_xlabel('Points per Race')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].set_title('Points Distribution')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(alpha=0.3)\n",
    "\n",
    "# Grid position\n",
    "axes[0,1].hist(df['grid'].dropna(), bins=24, edgecolor='black', alpha=0.7)\n",
    "axes[0,1].axvline(df['grid'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"grid\"].mean():.2f}')\n",
    "axes[0,1].axvline(df['grid'].median(), color='blue', linestyle='--', label=f'Median: {df[\"grid\"].median():.2f}')\n",
    "axes[0,1].set_xlabel('Grid Position')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_title('Grid Position Distribution')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(alpha=0.3)\n",
    "\n",
    "# Final position\n",
    "axes[1,0].hist(df['position_num'].dropna(), bins=24, edgecolor='black', alpha=0.7)\n",
    "axes[1,0].axvline(df['position_num'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"position_num\"].mean():.2f}')\n",
    "axes[1,0].axvline(df['position_num'].median(), color='blue', linestyle='--', label=f'Median: {df[\"position_num\"].median():.2f}')\n",
    "axes[1,0].set_xlabel('Final Position')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].set_title('Final Position Distribution')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(alpha=0.3)\n",
    "\n",
    "# Position change\n",
    "axes[1,1].hist(df['position_change'].dropna(), bins=40, edgecolor='black', alpha=0.7)\n",
    "axes[1,1].axvline(0, color='red', linestyle='-', linewidth=2, label='No change')\n",
    "axes[1,1].set_xlabel('Position Change (Grid - Final)')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].set_title('Position Change Distribution')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('analysis_results/fig1_distributions.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Saved: fig1_distributions.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Constructor Comparison Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating Figure 2: Constructor Comparison...\")\n",
    "\n",
    "# Top 8 constructors by total points\n",
    "top8_constructors = (df.groupby('constructor_name')['points']\n",
    "                     .sum()\n",
    "                     .sort_values(ascending=False)\n",
    "                     .head(8)\n",
    "                     .index.tolist())\n",
    "\n",
    "df_top8 = df[df['constructor_name'].isin(top8_constructors)].copy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_top8, x='constructor_name', y='points', \n",
    "            order=top8_constructors, palette='Set2')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Constructor', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Points per Race', fontsize=12, fontweight='bold')\n",
    "plt.title('Points Distribution by Constructor (Top 8, 2010-2024)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('analysis_results/fig2_constructor_boxplot.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Saved: fig2_constructor_boxplot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Yearly Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating Figure 3: Yearly Trends...\")\n",
    "\n",
    "yearly_stats = df.groupby('year').agg({\n",
    "    'points': ['sum', 'mean', 'std'],\n",
    "    'raceId': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "yearly_stats.columns = ['year', 'total_points', 'avg_points', 'std_points', 'num_races']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Formula 1 Trends Over Years (2010-2024)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Total points\n",
    "ax1.plot(yearly_stats['year'], yearly_stats['total_points'], \n",
    "         marker='o', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Year', fontweight='bold')\n",
    "ax1.set_ylabel('Total Points Awarded', fontweight='bold')\n",
    "ax1.set_title('Total Points Awarded per Season')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Average points\n",
    "ax2.plot(yearly_stats['year'], yearly_stats['avg_points'], \n",
    "         marker='o', linewidth=2, markersize=6, color='orange')\n",
    "ax2.set_xlabel('Year', fontweight='bold')\n",
    "ax2.set_ylabel('Average Points per Race', fontweight='bold')\n",
    "ax2.set_title('Average Points per Race')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('analysis_results/fig3_yearly_trends.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Saved: fig3_yearly_trends.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Top Drivers Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating Figure 4: Top Drivers...\")\n",
    "\n",
    "# Top 10 drivers\n",
    "top10_drivers = (df.groupby('driver_name')['points']\n",
    "                 .sum()\n",
    "                 .sort_values(ascending=True)\n",
    "                 .tail(10))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(top10_drivers)), top10_drivers.values, color='steelblue')\n",
    "plt.yticks(range(len(top10_drivers)), top10_drivers.index)\n",
    "plt.xlabel('Total Championship Points', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Driver', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 10 Drivers by Total Points (2010-2024)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(top10_drivers.values):\n",
    "    plt.text(v + 50, i, f'{v:.1f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('analysis_results/fig4_top_drivers.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Saved: fig4_top_drivers.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating Figure 5: Correlation Heatmap...\")\n",
    "\n",
    "# Select numeric variables\n",
    "corr_vars = ['grid', 'position_num', 'points', 'age_at_race']\n",
    "corr_data = df[corr_vars].dropna()\n",
    "corr_matrix = corr_data.corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix of Key Variables', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('analysis_results/fig5_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Saved: fig5_correlation_heatmap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Simple Regression Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating Figure 6: Simple Regression...\")\n",
    "\n",
    "# Prepare data\n",
    "df_reg = df[(df['grid'].notna()) & (df['position_num'].notna())].copy()\n",
    "X = df_reg[['grid']]\n",
    "y = df_reg['position_num']\n",
    "\n",
    "# Fit model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Simple Linear Regression: Grid Position ‚Üí Final Position', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Scatter plot with regression line\n",
    "ax1.scatter(df_reg['grid'], df_reg['position_num'], alpha=0.3, s=10)\n",
    "ax1.plot(df_reg['grid'], y_pred, color='red', linewidth=2, \n",
    "         label=f'y = {model.intercept_:.2f} + {model.coef_[0]:.2f}x')\n",
    "ax1.set_xlabel('Grid Position', fontweight='bold')\n",
    "ax1.set_ylabel('Final Position', fontweight='bold')\n",
    "ax1.set_title(f'Regression Line (R¬≤ = {r2_score(y, y_pred):.4f})')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals = y - y_pred\n",
    "ax2.scatter(y_pred, residuals, alpha=0.3, s=10)\n",
    "ax2.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Predicted Position', fontweight='bold')\n",
    "ax2.set_ylabel('Residuals', fontweight='bold')\n",
    "ax2.set_title('Residual Plot')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('analysis_results/fig6_simple_regression.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Saved: fig6_simple_regression.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Logistic Regression Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating Figure 7: Logistic Regression...\")\n",
    "\n",
    "# Prepare data\n",
    "df_logit = df[(df['grid'].notna()) & (df['podium'].notna())].copy()\n",
    "X = df_logit[['grid']]\n",
    "y = df_logit['podium']\n",
    "\n",
    "# Split and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Logistic Regression: Predicting Podium from Grid Position', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Logistic curve\n",
    "grid_range = np.linspace(1, 24, 100).reshape(-1, 1)\n",
    "proba = model.predict_proba(grid_range)[:, 1]\n",
    "\n",
    "ax1.scatter(df_logit['grid'], df_logit['podium'], alpha=0.1, s=5)\n",
    "ax1.plot(grid_range, proba, color='red', linewidth=3, label='Logistic Curve')\n",
    "ax1.set_xlabel('Grid Position', fontweight='bold')\n",
    "ax1.set_ylabel('Probability of Podium', fontweight='bold')\n",
    "ax1.set_title('Logistic Regression Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2, \n",
    "            xticklabels=['No Podium', 'Podium'],\n",
    "            yticklabels=['No Podium', 'Podium'])\n",
    "ax2.set_xlabel('Predicted', fontweight='bold')\n",
    "ax2.set_ylabel('Actual', fontweight='bold')\n",
    "ax2.set_title(f'Confusion Matrix (Accuracy: {accuracy_score(y_test, y_pred):.2%})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('analysis_results/fig7_logistic_regression.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Saved: fig7_logistic_regression.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Export Results\n",
    "\n",
    "### 7.1 Save Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüíæ Exporting summary tables...\")\n",
    "\n",
    "# Table 1: Descriptive Statistics\n",
    "desc_stats_table = df[['points', 'grid', 'position_num', 'age_at_race']].describe().T\n",
    "desc_stats_table = desc_stats_table[['count', 'mean', '50%', 'std', 'min', 'max']]\n",
    "desc_stats_table.columns = ['N', 'Mean', 'Median', 'SD', 'Min', 'Max']\n",
    "desc_stats_table.to_csv('analysis_results/table1_descriptive_stats.csv')\n",
    "print(\"‚úÖ Saved: table1_descriptive_stats.csv\")\n",
    "\n",
    "# Table 2: Hypothesis Tests Summary\n",
    "hypothesis_results = pd.DataFrame({\n",
    "    'Test': [\n",
    "        'One-Sample Proportion (Pole)',\n",
    "        'Independent t-test (Ham vs Ver)',\n",
    "        'One-Way ANOVA (Top 3 Teams)',\n",
    "        'Chi-Square (Grid vs Podium)',\n",
    "        'Pearson Correlation (Grid-Pos)'\n",
    "    ],\n",
    "    'Test_Statistic': ['z=0.401', 't=1.317', 'F=16.36', 'œá¬≤=2108.11', 'r=0.758'],\n",
    "    'p_value': [0.344, 0.189, '<0.001', '<0.001', '<0.001'],\n",
    "    'Decision': [\n",
    "        'Fail to reject H‚ÇÄ',\n",
    "        'Fail to reject H‚ÇÄ',\n",
    "        'Reject H‚ÇÄ',\n",
    "        'Reject H‚ÇÄ',\n",
    "        'Reject H‚ÇÄ'\n",
    "    ],\n",
    "    'Interpretation': [\n",
    "        'No evidence of >50% win rate',\n",
    "        'No difference between drivers',\n",
    "        'Significant difference among teams',\n",
    "        'Strong association (V=0.572)',\n",
    "        'Strong positive correlation'\n",
    "    ]\n",
    "})\n",
    "hypothesis_results.to_csv('analysis_results/table2_hypothesis_tests.csv', index=False)\n",
    "print(\"‚úÖ Saved: table2_hypothesis_tests.csv\")\n",
    "\n",
    "# Table 3: Regression Summary\n",
    "regression_results = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Simple Linear Regression',\n",
    "        'Multiple Linear Regression',\n",
    "        'Logistic Regression'\n",
    "    ],\n",
    "    'Equation': [\n",
    "        'Position = 2.42 + 0.66(Grid)',\n",
    "        'Points = 14.55 - 0.77(Grid) + 1.73(Merc) + 1.67(RB)',\n",
    "        'logit(P) = 1.38 - 0.47(Grid)'\n",
    "    ],\n",
    "    'R¬≤_or_Accuracy': [0.574, 0.193, 0.914],\n",
    "    'RMSE_or_F1': [3.48, 7.42, 0.650],\n",
    "    'Sample_Size': [5337, 1830, 1271],\n",
    "    'Key_Finding': [\n",
    "        'Grid explains 57.4% of final position variance',\n",
    "        'Teams add 1.7 points advantage',\n",
    "        'Grid strongly predicts podium probability'\n",
    "    ]\n",
    "})\n",
    "regression_results.to_csv('analysis_results/table3_regression_summary.csv', index=False)\n",
    "print(\"‚úÖ Saved: table3_regression_summary.csv\")\n",
    "\n",
    "# Table 4: Top 10 Drivers\n",
    "top_drivers.to_csv('analysis_results/table4_top_drivers.csv', index=False)\n",
    "print(\"‚úÖ Saved: table4_top_drivers.csv\")\n",
    "\n",
    "# Table 5: Top 10 Constructors\n",
    "top_constructors.to_csv('analysis_results/table5_top_constructors.csv', index=False)\n",
    "print(\"‚úÖ Saved: table5_top_constructors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# FINAL SUMMARY & DATA SCIENCE BEST PRACTICES\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üéâ ANALYSIS COMPLETE!\")\nprint(\"=\"*80)\n\nprint(\"\\nüìä FILES GENERATED:\")\nprint(\"-\" * 80)\nprint(\"  Visualizations (7):\")\nprint(\"    ‚Ä¢ fig1_distributions.png\")\nprint(\"    ‚Ä¢ fig2_constructor_boxplot.png\")\nprint(\"    ‚Ä¢ fig3_yearly_trends.png\")\nprint(\"    ‚Ä¢ fig4_top_drivers.png\")\nprint(\"    ‚Ä¢ fig5_correlation_heatmap.png\")\nprint(\"    ‚Ä¢ fig6_simple_regression.png\")\nprint(\"    ‚Ä¢ fig7_logistic_regression.png\")\nprint(\"\\n  Data Tables (5):\")\nprint(\"    ‚Ä¢ table1_descriptive_stats.csv\")\nprint(\"    ‚Ä¢ table2_hypothesis_tests.csv\")\nprint(\"    ‚Ä¢ table3_regression_summary.csv\")\nprint(\"    ‚Ä¢ table4_top_drivers.csv\")\nprint(\"    ‚Ä¢ table5_top_constructors.csv\")\nprint(\"\\n  Cleaned Dataset:\")\nprint(\"    ‚Ä¢ f1_modern_cleaned.csv\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üéØ KEY FINDINGS\")\nprint(\"=\"*80)\n\nfindings = [\n    (\"Grid Position Effect\", \n     \"Grid position explains 57.4% of final position variance (r=0.758, p<0.001)\"),\n    \n    (\"Starting Position Advantage\", \n     \"Top 3 grid positions achieve 60.9% podium rate vs 5.6% for others (œá¬≤=2108, p<0.001)\"),\n    \n    (\"Constructor Performance\", \n     \"Mercedes & Red Bull significantly outperform Ferrari (ANOVA F=16.36, p<0.001)\"),\n    \n    (\"Driver Comparison\", \n     \"No statistical difference between Hamilton & Verstappen (t=1.32, p=0.189)\"),\n    \n    (\"Pole Position\", \n     \"51.1% win rate from pole - not significantly >50% (z=0.40, p=0.344)\"),\n    \n    (\"Podium Prediction\", \n     \"Grid position predicts podium with 91.4% accuracy (Logistic AUC=0.91)\")\n]\n\nfor i, (title, finding) in enumerate(findings, 1):\n    print(f\"\\n{i}. {title}\")\n    print(f\"   ‚Üí {finding}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ DATA SCIENCE BEST PRACTICES IMPLEMENTED\")\nprint(\"=\"*80)\n\npractices = [\n    (\"Reproducibility\", [\n        \"Fixed random seed (RANDOM_STATE=42)\",\n        \"All analysis steps documented\",\n        \"Code organized in logical sections\",\n        \"Version control ready\"\n    ]),\n    \n    (\"Data Quality\", [\n        \"Comprehensive validation functions\",\n        \"Missing value analysis & reporting\",\n        \"Duplicate detection\",\n        \"Outlier investigation\"\n    ]),\n    \n    (\"Statistical Rigor\", [\n        \"Assumptions tested before each test\",\n        \"Effect sizes reported (not just p-values)\",\n        \"Confidence intervals provided\",\n        \"Post-hoc tests for ANOVA\",\n        \"Multiple test corrections\"\n    ]),\n    \n    (\"Model Validation\", [\n        \"Train-test split (80/20)\",\n        \"Stratified sampling for classification\",\n        \"Cross-validation (5-fold)\",\n        \"Overfitting detection\",\n        \"Regression diagnostics (4 assumptions)\",\n        \"Multicollinearity check (VIF)\"\n    ]),\n    \n    (\"Comprehensive Metrics\", [\n        \"Classification: Acc, Prec, Rec, F1, AUC-ROC\",\n        \"Regression: R¬≤, Adj-R¬≤, RMSE, MAE\",\n        \"Both training & test performance\",\n        \"Confusion matrices with interpretation\"\n    ]),\n    \n    (\"Professional Reporting\", [\n        \"Clear section headers\",\n        \"Formatted output tables\",\n        \"Interpretation sections\",\n        \"Publication-ready visualizations (300 DPI)\",\n        \"Exportable results\"\n    ])\n]\n\nfor category, items in practices:\n    print(f\"\\n‚úì {category}:\")\n    for item in items:\n        print(f\"  ‚Ä¢ {item}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìö STATISTICAL TECHNIQUES APPLIED\")\nprint(\"=\"*80)\n\nprint(\"\\nFrom TU155 (Statistics):\")\nprint(\"  ‚Ä¢ Descriptive Statistics (mean, median, SD, IQR)\")\nprint(\"  ‚Ä¢ One-Sample Proportion Test (z-test)\")\nprint(\"  ‚Ä¢ Independent Samples t-test\")\nprint(\"  ‚Ä¢ Hypothesis Testing Framework\")\nprint(\"  ‚Ä¢ Confidence Intervals\")\n\nprint(\"\\nFrom DSI204 (Data Science):\")\nprint(\"  ‚Ä¢ One-Way ANOVA with post-hoc tests\")\nprint(\"  ‚Ä¢ Chi-Square Test of Independence\")\nprint(\"  ‚Ä¢ Pearson Correlation\")\nprint(\"  ‚Ä¢ Simple Linear Regression\")\nprint(\"  ‚Ä¢ Multiple Linear Regression\")\nprint(\"  ‚Ä¢ Logistic Regression\")\nprint(\"  ‚Ä¢ Cross-Validation\")\nprint(\"  ‚Ä¢ Model Diagnostics\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìñ IMPROVEMENTS OVER ORIGINAL CODE\")\nprint(\"=\"*80)\n\nimprovements = [\n    \"Added comprehensive data validation and quality checks\",\n    \"Implemented statistical assumptions testing (normality, homoscedasticity, etc.)\",\n    \"Added effect size reporting (Cohen's d, eta-squared, Cram√©r's V)\",\n    \"Implemented train-test split and cross-validation\",\n    \"Added regression diagnostics (linearity, homoscedasticity, normality, independence)\",\n    \"Included multicollinearity detection (VIF)\",\n    \"Added proper classification metrics (AUC-ROC, confusion matrix details)\",\n    \"Implemented reproducibility (random seed, timestamps)\",\n    \"Structured code with clear sections and helper functions\",\n    \"Added comprehensive documentation and interpretation sections\",\n    \"Improved output formatting for readability\",\n    \"Added overfitting detection and model generalization checks\"\n]\n\nfor i, improvement in enumerate(improvements, 1):\n    print(f\"  {i:2d}. {improvement}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üöÄ READY FOR ACADEMIC SUBMISSION\")\nprint(\"=\"*80)\nprint(\"\\nThis analysis follows industry-standard data science practices and is\")\nprint(\"suitable for academic coursework, research papers, or professional portfolios.\")\nprint(\"\\n‚úì All assumptions documented\")\nprint(\"‚úì All models validated\")\nprint(\"‚úì All results reproducible\")\nprint(\"‚úì All findings interpretable\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üèéÔ∏èüí® Good Luck with Your F1 Analysis Project!\")\nprint(\"=\"*80 + \"\\n\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Statistical-Analysis-of-Performance-Factors-in-Formula-1-2010-2024-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}